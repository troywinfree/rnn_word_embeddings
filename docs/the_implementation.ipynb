{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Implementation\n",
    "\n",
    "I put the [theano](http://deeplearning.net/software/theano/) RNN model and training in two different classes: ```embedding_model``` and ```word_embedder```. An ```embedding_model``` builds and compiles theano functions for the RNN network, the loss, and the stochastic gradient descent ([SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)) update. A ```word_embedder``` is responsible for running training minibatches, initialized from data or pickle files.\n",
    "\n",
    "### The basics\n",
    "\n",
    "Let's get started with imports and globals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "\n",
    "# the activation function and softmax\n",
    "__f = T.nnet.sigmoid\n",
    "__y = T.nnet.softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I put the basic units of the [model](the_model.ipynb) in three functions: ```rnn```, ```compute_network_outputs```, and ```compute_mean_log_lklyhd_outputs```.  A single neuron of the model is quite simple and can be taken care of in one line of code (ignoring the comments and function definition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn(i_t,s_tm1,U,W,b) : \n",
    "    \"\"\" A single neuron in the recursive network\n",
    "    \n",
    "        Input\n",
    "        \n",
    "        i_t   - one-of-n index encoding\n",
    "        s_tm1 - output from the previous stage of the rnn\n",
    "        U     - weight matrix for x_t\n",
    "        W     - weight matrix for s_tm1\n",
    "        b     - bias vector\n",
    "        \n",
    "        Output\n",
    "        \n",
    "        s_t   - output of the t-th stage of the rnn\n",
    "    \"\"\"\n",
    "    \n",
    "    return __f(U[:,i_t] + W.dot(s_tm1) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally I had this function take in a vector ```x``` in place of the index ```i_t```. With the one-of-n encoding, ```x``` will always be a standard Euclidean basis vector consisting of a one at some index (```i_t```) and zeros everywhere else. So ```x``` amounts to a wildly inefficient encodding of the index ```i_t```. In fact this oversight made training cripplingly slow.\n",
    "\n",
    "The full network is produced by looping over a vector of one-of-n indices (representing a sentence) in the forward and backward directions summing the results, multipling by another weight matrix and passing that to the soft max function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_network_outputs(i,s0,V,U,W,b) : \n",
    "    \"\"\" Builds the recursive network using theano's scan iterator. This \n",
    "        is a bidirectional rnn where we simply add the outputs of the \n",
    "        rnns in both directions. \n",
    "        \n",
    "        Input\n",
    "        \n",
    "        i    - array of one-of-n indices representing a sentence\n",
    "        s0   - initial output of the rnn\n",
    "        V    - weight matrix for softmax step\n",
    "        U    - weight matrix for the input to the recursive neuron\n",
    "        W    - weight matrix for the previous output of the recursive neuron\n",
    "        b    - bias vector\n",
    "        \n",
    "        Output\n",
    "        \n",
    "        y    - uncompiled symbolic theano representation of the rnn\n",
    "    \"\"\"\n",
    "    \n",
    "    fwrd_rslt,_ = theano.scan(rnn,\n",
    "                              sequences = [i],\n",
    "                              outputs_info = [s0],\n",
    "                              non_sequences = [U,W,b])\n",
    "    bwrd_rslt,_ = theano.scan(rnn,\n",
    "                              sequences = [i[::-1]],\n",
    "                              outputs_info = [s0],\n",
    "                              non_sequences = [U,W,b])\n",
    "    \n",
    "    return __y(T.dot(fwrd_rslt + bwrd_rslt, V.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping in theano is handled by the [```scan```](http://deeplearning.net/software/theano/library/scan.html) function. I'd like to pause here for a moment and reflect on how cool theano is. In all honesty when I started playing around with theano it was somewhat revelatory. I am a technical person and I use Python to solve technical problems from all over the mathematical map every day at work. Many are optimization problems. And I have taken for granted the notion that the real meat of an optimization problem is programming the objective, and it's gradient and Hessian. For neural networks this translates to programming backpropagation. As far as theano is concerned however that way of thinking is incredibly naive (and I evidenty was). In theano the work, such as it is, is in setting up the function model graph, and whatever update you may need for your favorite flavor of SGD. Then theano takes care of the rest with exact (symbolic) differentiation. I'm getting a bit over-worked here but suffices to say that I think theano is cool.\n",
    "\n",
    "Returning to the code at hand. It's probably easiest to communicate what's going on in the calls to the ```scan``` function by translating one of them into a regular old Python loop. First the call to scan:\n",
    "\n",
    "```python\n",
    "fwrd_rslt,_ = theano.scan(rnn,\n",
    "                          sequences = [i],\n",
    "                          outputs_info = [s0],\n",
    "                          non_sequences = [U,W,b])\n",
    "```\n",
    "\n",
    "As a Python loop this would read\n",
    "\n",
    "```python\n",
    "data = np.zeros((len(i)+1,len(s0)))\n",
    "data[0] = s0\n",
    "for j,i_t in enumerate(i) : \n",
    "    data[j+1] = rnn(i_t,data[j],U,W,b)\n",
    "fwrd_rslt = data[1:]\n",
    "```\n",
    "\n",
    "So in this case the ```scan``` function is mapping the function ```rnn``` across the vector ```i``` with normal arguments ```U```, ```W```, and ```b``` and a special argument ```s_tm1``` whose type is the same as ```s0``` and whose value is the output of the call to ```rnn``` from the previous iteration, initialized with ```s0```.\n",
    "\n",
    "What about the second output of ```scan``` that in this case we are saving in the the throw-away variable '```_```'? As I understand it, that output is a dictionary of 'updates' for theano shared variables. Theano shared variables allow theano functions to have internal states, like counting variables or, as will be the case for us, weight matrices. A given function may update the value of a shared variable and theano needs to keep track of that in a special way. In the code above, there will be shared variables (```V```, ```U```, ```W```, and ```b```) but network evaluation does not update their values so the updates dictionary returned by ```scan``` is empty.\n",
    "\n",
    "For the loss we need to map ```compute_network_outputs``` across the test sentences (matrix of one-of-n indices) to produce the probabilities of the vocabulary words occuring at the various locations in sentences, and take the mean of the log of a requested collection of these probabilities. The code is likely a bit more clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mean_log_lklyhd_outputs(I,J,s0,V,U,W,b) : \n",
    "    \"\"\" Builds theano symbolic representation of the mean log likelyhood\n",
    "        loss function for the embedding model. \n",
    "        \n",
    "        Input\n",
    "    \n",
    "        I    - matrix of one-of-n indices representing input sentences\n",
    "        J    - matrix of one-of-n indices representing target sentences\n",
    "        s0   - initial output of the rnn\n",
    "        V    - weight matrix for softmax step\n",
    "        U    - weight matrix for the input to the recursive neuron\n",
    "        W    - weight matrix for the previous output of the recursive neuron\n",
    "        b    - bias vector\n",
    "        \n",
    "        Output\n",
    "        \n",
    "        uncompiled symbolic theano representation of mean log likelyhood loss\n",
    "    \"\"\"\n",
    "        \n",
    "    # the function to scan over - it just collects the log likelyhood of \n",
    "    # the positions indicated by j given i and the weight matrices\n",
    "    def scan_f(i,j,s0,V,U,W,b) : \n",
    "        \n",
    "        outs = compute_network_outputs(i,s0,V,U,W,b)\n",
    "        \n",
    "        return T.log(theano.scan(lambda j_t,o : o[j_t],\n",
    "                                 sequences = [j,outs])[0]) \n",
    "    \n",
    "    batch_outputs,_ = theano.scan(scan_f,\n",
    "                                  sequences = [I,J],\n",
    "                                  non_sequences = [s0,V,U,W,b])\n",
    "    \n",
    "    return T.mean(batch_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```embedding_model```\n",
    "\n",
    "The ```embedding_model``` object collects the theano symbolic representations for the network, the loss, the gradient and the SGD update, and lets you compile (call theano's ```function``` method) each as needed. Each embedding model instance will depend on the size of the problem vocabulary, ```n```, and the dimension of the embedding space, ```k```. The version of this class discussed here is a slighly pared down version of the one in the repository. Here is the ```___init___``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class embedding_model : \n",
    "    \"\"\" Bidirectional RNN for experimenting with word embeddings. The model is\n",
    "        the following\n",
    "           \n",
    "        s_+(t) = f(Uw(t) + Ws_+(t-1) + b)\n",
    "        s_-(t) = f(Uw(m-t) + Ws_-(t-1) + b)\n",
    "        y(t) = g(Vs_+(t) + Vs_-(t))\n",
    "        \n",
    "        and\n",
    "        \n",
    "        f(z) = 1 / (1 + e^{-z})\n",
    "        g(z_m) = e^{z_m} / sum e^{z_k}\n",
    "        \n",
    "        This is just a bidirectional version of the model that appears in \n",
    "        https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n,   # dimension of inputs\n",
    "                 k,   # dimension of embedding space\n",
    "                 seed = None,\n",
    "                 s0 = None,\n",
    "                 int_dtype = 'int64',\n",
    "                 float_dtype = floatX\n",
    "                 ) : \n",
    "        \"\"\" Model initializer\n",
    "        \n",
    "            Inputs\n",
    "            \n",
    "            n     - dimension of the inputs at each recursive layer\n",
    "            k     - dimension of the embedding space\n",
    "            seed  - optional seed for random initialization of weight matrices\n",
    "            s0    - optional initial output to start netword recursion\n",
    "            dtype - float type for computations\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.seed = seed\n",
    "        self.int_dtype = int_dtype\n",
    "        self.float_dtype = float_dtype\n",
    "        \n",
    "        # seed the random generator if requested\n",
    "        if seed is not None : \n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # collect s0\n",
    "        if s0 is None : \n",
    "            self.s0 = theano.shared(np.zeros(k,dtype = self.float_dtype),name='s0')\n",
    "        else : \n",
    "            self.s0 = theano.shared(np.array(s0,dtype = self.float_dtype),name='s0')\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.V = theano.shared(np.array(np.random.rand(n,k),\n",
    "                                        dtype = self.float_dtype),name='V')\n",
    "        self.U = theano.shared(np.array(np.random.rand(k,n),\n",
    "                                        dtype = self.float_dtype),name='U')\n",
    "        # initializing with full matrix can lead to vanishing gradients\n",
    "        self.W = theano.shared(np.array(np.diag(np.random.rand(k)),\n",
    "                                        dtype=self.float_dtype),name='W')\n",
    "        self.b = theano.shared(np.array(np.random.rand(k),\n",
    "                                        dtype = self.float_dtype),name='b')  \n",
    "        \n",
    "        # shared variables for mean gradient magnitudes\n",
    "        self.m_dV_mag = theano.shared(np.array(0.,dtype = self.float_dtype),\n",
    "                                      name='m_dV_mag')\n",
    "        self.m_dU_mag = theano.shared(np.array(0.,dtype = self.float_dtype),\n",
    "                                      name='m_dU_mag')\n",
    "        self.m_dW_mag = theano.shared(np.array(0.,dtype = self.float_dtype),\n",
    "                                      name='m_dW_mag')\n",
    "        self.m_db_mag = theano.shared(np.array(0.,dtype = self.float_dtype),\n",
    "                                      name='m_db_mag')\n",
    "        \n",
    "        # shared variables for computing the loss\n",
    "        self.loss_accum = theano.shared(np.array(0.,dtype = self.float_dtype),\n",
    "                                        name='loss_accum')\n",
    "        self.loss_accum_i = theano.shared(np.array(0.,dtype = self.float_dtype),\n",
    "                                          name='loss_accum_i')\n",
    "        \n",
    "        # compute the network\n",
    "        self._compute_network_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ```__init__``` method, all the theano shared variables are initialized and then the ```_compute_network_model``` method is called. In addition to shared variables for the weights which will be updated in each SGD iteration during training, there are also variables for the mean, across SGD updates, of the gradient magnitudes of the weights (```m_d```*```_mag```), and variables for computing the loss. I've set up the loss in a way that allows it to be computed in a series of batches. This arose out of the silly way I originally implemented ```rnn``` (as mentioned above, with full one-of-n vector inputs) which led to Python kernel death if I passed all the test data to the loss function. With the implementation of ```rnn``` given above there is no need for loss evaluation across batches. \n",
    "\n",
    "Notice that I am initializing ```W```, the weights on the output from the previous recursive layer, with a random diagonal matrix. This is in response to the observation that ```W``` gradients vanish if initialization is done naively with a dense random matrix.\n",
    "\n",
    "The ```_compute_network_model``` method builds all the required theano function graphs. Note that in order to split the definition of the class up across several Jupyter notebook cells I am defining the following methods as Python functions and then binding them to the ```embedding_model``` class (which is the content of the last line of the following cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _compute_network_model(self) : \n",
    "    \"\"\" Build the network, loss, grad_loss and sgd_update theano functions.\n",
    "        More work than is strictly nessecary is done here as the only thing\n",
    "        that is really needed in order to run sgd (stochastic gradient \n",
    "        descent) is the sgd_update function. The network, loss and grad_loss\n",
    "        functions are compiled since this is experimental code.\n",
    "    \"\"\"\n",
    "\n",
    "    # build the network\n",
    "    self.i = T.vector('i',dtype = self.int_dtype)\n",
    "\n",
    "    self.network_outputs = compute_network_outputs(self.i,self.s0,self.V,\n",
    "                                                   self.U,self.W,self.b)\n",
    "\n",
    "\n",
    "    # build mean log likelyhood loss\n",
    "\n",
    "    # variables for a batch of sentences\n",
    "    self.I = T.matrix('I',dtype = self.int_dtype)\n",
    "    self.J = T.matrix('J',dtype = self.int_dtype) # for embedding I = J\n",
    "\n",
    "    self.loss_outputs = compute_mean_log_lklyhd_outputs(self.I,self.J,\n",
    "                                                        self.s0,self.V,\n",
    "                                                        self.U,self.W,\n",
    "                                                        self.b)\n",
    "\n",
    "    # set up the accumulator for computing the loss in batches\n",
    "\n",
    "    n_minibatch = T.cast(self.I.shape[0],self.float_dtype)\n",
    "    loss_accum_ipnm = self.loss_accum_i + n_minibatch\n",
    "\n",
    "    self.loss_updates = ((self.loss_accum,\n",
    "                          (self.loss_outputs*n_minibatch/loss_accum_ipnm\n",
    "                           + (self.loss_accum \n",
    "                             * self.loss_accum_i/loss_accum_ipnm))),\n",
    "                         (self.loss_accum_i,loss_accum_ipnm))\n",
    "\n",
    "    # get the gradient of the loss\n",
    "\n",
    "    (self.dV,\n",
    "     self.dU,\n",
    "     self.dW,\n",
    "     self.db) = theano.grad(self.loss_outputs,\n",
    "                            [self.V,self.U,self.W,self.b])\n",
    "\n",
    "    # get the gradient magnitudes\n",
    "\n",
    "    self.dV_mag = T.sqrt(T.sum(self.dV*self.dV))\n",
    "    self.dU_mag = T.sqrt(T.sum(self.dU*self.dU))\n",
    "    self.dW_mag = T.sqrt(T.sum(self.dW*self.dW))\n",
    "    self.db_mag = T.sqrt(T.sum(self.db*self.db))\n",
    "\n",
    "    # get the sgd update function\n",
    "\n",
    "    # this is the learning parameter\n",
    "    self.eta = T.scalar('eta',dtype = self.float_dtype)\n",
    "\n",
    "    # also including a running average of the gradient magnitudes\n",
    "\n",
    "    self.sgd_i = T.scalar('sgd_i',dtype = self.float_dtype)\n",
    "\n",
    "    dV_mag_accum = (self.dV_mag/(self.sgd_i+1.)\n",
    "                        + self.m_dV_mag*(self.sgd_i/(self.sgd_i+1.)))\n",
    "    dU_mag_accum = (self.dU_mag/(self.sgd_i+1.) \n",
    "                        + self.m_dU_mag*(self.sgd_i/(self.sgd_i+1.)))\n",
    "    dW_mag_accum = (self.dW_mag/(self.sgd_i+1.) \n",
    "                        + self.m_dW_mag*(self.sgd_i/(self.sgd_i+1.)))\n",
    "    db_mag_accum = (self.db_mag/(self.sgd_i+1.) \n",
    "                        + self.m_db_mag*(self.sgd_i/(self.sgd_i+1.)))\n",
    "\n",
    "    # adding here since we are taking a max of the loss - accumulators\n",
    "    # do not include the latest values\n",
    "    self.sgd_updates = ((self.V,self.V + self.eta*self.dV),\n",
    "                        (self.U,self.U + self.eta*self.dU),\n",
    "                        (self.W,self.W + self.eta*self.dW),\n",
    "                        (self.b,self.b + self.eta*self.db),\n",
    "                        (self.m_dV_mag,dV_mag_accum),\n",
    "                        (self.m_dU_mag,dU_mag_accum),\n",
    "                        (self.m_dW_mag,dW_mag_accum),\n",
    "                        (self.m_db_mag,db_mag_accum))\n",
    "\n",
    "    # pointers for the compiled functions\n",
    "    self.network = None\n",
    "    self.loss = None\n",
    "    self.grad_loss = None\n",
    "    self.sgd_update = None\n",
    "    self.sgd_update_w_loss = None\n",
    "\n",
    "# bind the method to the class - this is just so the class definition\n",
    "# can be broken up across several Jupyter notebook cells\n",
    "embedding_model._compute_network_model = _compute_network_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the network and loss graphs are built in ```compute_network_outputs``` and ```compute_mean_log_lklyhd_outputs```. Theano's ```grad``` function makes computing derivatives trivial. The efforts I undertake with the  *```_accum``` variables might look abit cumbersome, but all I am doing is multiplying by the number of summands in the previous accumulated mean, adding the new summands and dividing everything by the new total. For the loss accumulation I am keeping the number of summands in the last loss evaluation in the variable ```self.loss_accum_i```. For the mean gradient magnitude accumulation the corresponding quantity is the number of minibatches so far, I am keeping that number in ```self.sgd_i```. \n",
    "\n",
    "One other thing deserves note: since we are doing maximum likelyhood, the SGD update *adds* the scaled gradient to the weight matrices. Typically SGD is stated for a minimization problem, in which case the right thing to do is subtract a scaled version of the gradient. So in our case we are really doing stochastic gradient ascent. \n",
    "\n",
    "Next we have the methods that call theano's ```function``` method to actually compile the graphs so data can be passed through them. Below I've only included compile methods for the network, the loss, and the SGD update. The code in the repository also has methods for compiling the gradient of the loss and a version of the SGD update that in addition to running the update also returns the loss. As before, in order to split the class definition across multiple Jupyter cells the following methods are defined as Python functions and then bound to the ```embedding_model``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_network(self) : \n",
    "    \"\"\" Compile the network  \n",
    "    \"\"\"\n",
    "    if self.network is not None : \n",
    "        return\n",
    "\n",
    "    self.network = theano.function(inputs = [self.i],\n",
    "                                   outputs = self.network_outputs)\n",
    "\n",
    "# bind the method to the class\n",
    "embedding_model.compile_network = compile_network\n",
    "\n",
    "def compile_loss(self) : \n",
    "    \"\"\" Compile the loss\n",
    "    \"\"\"\n",
    "\n",
    "    if self.loss is not None : \n",
    "        return\n",
    "\n",
    "    self.loss = theano.function(inputs = [self.I,self.J],\n",
    "                                outputs = self.loss_outputs,\n",
    "                                updates = self.loss_updates)\n",
    "\n",
    "# bind the method to the class\n",
    "embedding_model.compile_loss = compile_loss\n",
    "        \n",
    "def compile_sgd_update(self) : \n",
    "    \"\"\" Compile SGD update function\n",
    "    \"\"\"\n",
    "\n",
    "    if self.sgd_update is not None : \n",
    "        return\n",
    "\n",
    "    self.sgd_update = theano.function(inputs = [self.I,self.J,\n",
    "                                                self.eta,self.sgd_i],\n",
    "                                      outputs = [],\n",
    "                                      updates = self.sgd_updates)\n",
    "\n",
    "# bind the method to the class\n",
    "embedding_model.compile_sgd_update = compile_sgd_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano's ```function``` method takes in the inputs and outputs of the desired function, together with any desired updates on shared varialbes. Evaluating the network does not involve shared variable updates. In the SGD update however the whole point is to update the weights of the model, so in ```compile_sgd_update``` there are no outputs in the call to ```function```, only inputs and updates.\n",
    "\n",
    "Now we have enough in place to initialize an example model and play around with some fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18455518,  0.05965285,  0.17996989,  0.3272838 ,  0.24853828],\n",
       "       [ 0.18423066,  0.05006433,  0.1724503 ,  0.34385194,  0.24940277],\n",
       "       [ 0.18613941,  0.04683204,  0.16852475,  0.35007937,  0.24842442],\n",
       "       [ 0.18665375,  0.04682157,  0.16826569,  0.35010207,  0.24815692],\n",
       "       [ 0.18246037,  0.04999981,  0.17327421,  0.34394022,  0.25032539],\n",
       "       [ 0.18199794,  0.05041981,  0.17388096,  0.34314667,  0.25055461],\n",
       "       [ 0.18616735,  0.04726935,  0.16892835,  0.34921873,  0.24841623],\n",
       "       [ 0.1863764 ,  0.04651053,  0.16810058,  0.35071763,  0.24829486],\n",
       "       [ 0.18460649,  0.04865383,  0.17098595,  0.34652341,  0.24923032],\n",
       "       [ 0.18221862,  0.04946176,  0.17290928,  0.34494665,  0.2504637 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a model\n",
    "n_vocab = 5\n",
    "dim_embedding = 2\n",
    "seed = 12345\n",
    "model = embedding_model(n_vocab,dim_embedding,seed)\n",
    "\n",
    "# compile the network\n",
    "model.compile_network()\n",
    "\n",
    "# make some dummy data\n",
    "n_words_in_sent = 10\n",
    "sentence = np.array([np.random.randint(n_vocab) for i in range(n_words_in_sent)])\n",
    "\n",
    "# evaluate the network on the dummy data\n",
    "model.network(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some fake data and evaluate the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-1.798485079447346)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the loss\n",
    "model.compile_loss()\n",
    "\n",
    "# make some fake training data\n",
    "n_sentences = 100\n",
    "in_sentences = np.array([[np.random.randint(n_vocab) \n",
    "                              for i in range(n_words_in_sent)]\n",
    "                                                for j in range(n_sentences)])\n",
    "out_sentences = np.array([[np.random.randint(n_vocab) \n",
    "                              for i in range(n_words_in_sent)]\n",
    "                                                for j in range(n_sentences)])\n",
    "\n",
    "# evaluate the loss on the fake data\n",
    "model.loss(in_sentences,out_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's run a couple SGD updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatch 1\n",
      "mean dV mag = 0.5222211706893597\n",
      "mean dU mag = 0.022116767224227115\n",
      "mean dW mag = 0.04942633568319734\n",
      "mean db mag = 0.04899231231942744\n",
      "\n",
      "\n",
      "minibatch 2\n",
      "mean dV mag = 0.5136943284427516\n",
      "mean dU mag = 0.023094645546331213\n",
      "mean dW mag = 0.049932791733365636\n",
      "mean db mag = 0.0504770132400983\n"
     ]
    }
   ],
   "source": [
    "# compile the SGD update\n",
    "model.compile_sgd_update()\n",
    "\n",
    "# do SGD on the fake data \n",
    "\n",
    "minibatch_size = 50\n",
    "eta = 0.001 # learning rate\n",
    "\n",
    "i_n = int(np.ceil(n_sentences/minibatch_size))\n",
    "for i in range(i_n) :\n",
    "\n",
    "    # get the batch data\n",
    "    in_batch = in_sentences[i*minibatch_size:(i+1)*minibatch_size]\n",
    "    out_batch = out_sentences[i*minibatch_size:(i+1)*minibatch_size]\n",
    "    \n",
    "    # do the update and get the loss - \n",
    "    model.sgd_update(in_batch,out_batch,eta,i)\n",
    "    \n",
    "    print(\"minibatch\", i+1)\n",
    "    print(\"mean dV mag =\",model.m_dV_mag.get_value())\n",
    "    print(\"mean dU mag =\",model.m_dU_mag.get_value())\n",
    "    print(\"mean dW mag =\",model.m_dW_mag.get_value())\n",
    "    print(\"mean db mag =\",model.m_db_mag.get_value())\n",
    "    if i != i_n-1 : print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's the implementation of the RNN model. There is at least one thing that I think could be improved: I would like to factor out the SGD update. Philosophically, the gradient descent update does not belong in the model since it is not part of the RNN. Practically, if the SGD update is factored out of the model then supporting other flavors of SGD (AdaGrad, Adam, etc.) would be more natural.\n",
    "\n",
    "### ```word_embedder```\n",
    "\n",
    "The ```word_embedder``` object deals with reading sentence data, reading and writing training states, and with training. Sentence data is read from a numpy npz file, from which unique words are extracted and sorted by frequency. The top ```n_vocabulary-2``` most frequent words are taken as the vocabulary, together with two special tokens, one indicating the end of a sentence and another indicating an unknown word. The sentences are then translated into one-of-n indices with respect to the vocabulary and separated into training and testing sets. \n",
    "\n",
    "As with the ```embedding_model``` class given above, the ```word_embedder``` class discussed below is a stripped-down version of the one in the repository. Namely the one given here does not include the methods for reading/writing a training state from/to a pickle file. \n",
    "\n",
    "I put sentence reading and one-of-n encoding into the following two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def read_sentences(path,n_vocabulary) : \n",
    "    \"\"\" Read sentences from the disk and collect the words, word counts\n",
    "        and vocabulary. \n",
    "        \n",
    "        Sentence data is assumed to start with a '<s>' token and end with \n",
    "        a list of '<e>' tokens. The top n_vocabulary most common words are \n",
    "        extracted from the sentence data to form the vocabulary. Of these, the\n",
    "        least common word is replaced with a '<u>' (unknown) token and if\n",
    "        '<e>' is not present in the vocabulary the second least common word \n",
    "        is replaced with '<e>'.\n",
    "        \n",
    "        Inputs : \n",
    "            \n",
    "        path         - path to the sentence .npz file\n",
    "        n_vocabulary - top n_vocabulary most common words will form the vocab\n",
    "        \n",
    "        Outputs : \n",
    "            \n",
    "        sentences    - array of sentences read from the disk\n",
    "        words        - all the words present in the sentences\n",
    "        counts       - the number of occurances of each word in the data\n",
    "        vocabulary   - the vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    if n_vocabulary < 2 : \n",
    "        raise ValueError('Need more than two words in vocabulary')\n",
    "    \n",
    "    # load the sentences - not including start symbol\n",
    "    sentences = np.load(path)['arr_0'][:,1:]\n",
    "\n",
    "    # get the words and word counts\n",
    "    words,counts = np.unique(sentences,return_counts = True)\n",
    "    \n",
    "    # get the sort map\n",
    "    sort_map = np.argsort(counts)\n",
    "    \n",
    "    # collect the vocabulary\n",
    "    \n",
    "    vocabulary = np.zeros(n_vocabulary,dtype = words.dtype)\n",
    "    vocabulary = words[sort_map][-n_vocabulary:][::-1]\n",
    "\n",
    "    if '<e>' not in vocabulary : \n",
    "        vocabulary[-2] = '<e>'\n",
    "        vocabulary[-1] = '<u>'\n",
    "    else : \n",
    "        vocabulary[-1] = '<u>'\n",
    "        \n",
    "    return (sentences,words,counts,vocabulary)\n",
    "        \n",
    "\n",
    "def get_one_of_n_indices(vocabulary,sentences,\n",
    "                         dtype = np.int16) : \n",
    "    \"\"\" Translate the sentence data into index data - each word is replaced\n",
    "        by its index in the vocabulary array\n",
    "        \n",
    "        Inputs : \n",
    "            \n",
    "        vocabulary   - array of vocabulary words\n",
    "        sentences    - array of sentences\n",
    "        dtype        - signed integer type for the output array\n",
    "        \n",
    "        Outputs : \n",
    "            \n",
    "        index array  - array in which the words in each sentence in the\n",
    "                       sentences array are replaced by their corresponding\n",
    "                       index in the vocabulary array\n",
    "    \"\"\"\n",
    "      \n",
    "    if '<u>' != vocabulary[-1] : \n",
    "        raise ValueError(\"Last member of vocabulary should be '<u>' token\")\n",
    "    \n",
    "    n_vocab = len(vocabulary)\n",
    "\n",
    "    if np.array(n_vocab,dtype=dtype) != n_vocab : \n",
    "        raise ValueError(\"Provided dtype cannot represent size of vocabulary\")\n",
    "    if np.array(-1,dtype=dtype) != -1 : \n",
    "        raise ValueError(\"Provided dtype must be signed\")\n",
    "    \n",
    "    # using counter since if a key is not in a counter\n",
    "    # it returns zero, so unknown tokens will return zero\n",
    "    vocab_map = Counter(dict((token,i+1) \n",
    "                                for i,token in enumerate(vocabulary[:-1])))\n",
    "\n",
    "    return np.array([[vocab_map[token]-1 for token in sentence]\n",
    "                                     for sentence in sentences],dtype = dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ```word_embedder``` object should be initialized from a sentence npz or from a pickled training state. The following ```__init__``` method is used inside those operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# print all logging statements to stdout\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s',\n",
    "                    level=logging.DEBUG)\n",
    "\n",
    "\n",
    "class word_embedder : \n",
    "    \"\"\" Class for training a word embedding RNN\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary,\n",
    "                 training_data,\n",
    "                 test_data,\n",
    "                 model,\n",
    "                 minibatch_size,\n",
    "                 eval_test_loss_stride,\n",
    "                 n_test_loss_batches,\n",
    "                 seed = None,\n",
    "                 logger = logging\n",
    "                 ) :\n",
    "        \"\"\" word_embedder initializer\n",
    "        \n",
    "            Inputs : \n",
    "                \n",
    "            vocabulary            - words in the vocabulary, including '<e>'\n",
    "                                    and '<u>' tags (end-of-sentence, and unknown)\n",
    "            training_data         - training data as one-of-n indices to the\n",
    "                                    vocabulary array\n",
    "            test_data             - test data as one-of-n indices to the \n",
    "                                    vocabulary array\n",
    "            model                 - embedding model\n",
    "            minibatch_size        - number of sentences to use in a minibatch\n",
    "            eval_test_loss_stride - number of minibatches between loss test\n",
    "                                    loss evaluations\n",
    "            n_test_loss_batches   - number of batches over which to accumulate \n",
    "                                    test loss\n",
    "            seed                  - seed for random shuffle after each epoch,\n",
    "            logger                - logging object\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vocabulary = vocabulary\n",
    "        self.n_vocabulary = len(vocabulary)\n",
    "        self.training_data = training_data\n",
    "        self.training_data_size = len(training_data)\n",
    "        self.training_indices = np.arange(self.training_data_size)\n",
    "        self.test_data = test_data\n",
    "        self.test_data_size = len(self.test_data)\n",
    "        self.model = model\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.eval_test_loss_stride = eval_test_loss_stride\n",
    "        self.n_test_loss_batches = n_test_loss_batches\n",
    "        \n",
    "        self.test_loss_batch_size = self.test_data_size\n",
    "        self.test_loss_batch_size /= self.n_test_loss_batches\n",
    "        self.test_loss_batch_size = int(np.ceil(self.test_loss_batch_size))\n",
    " \n",
    "        self.seed = 0\n",
    "        if seed is not None : \n",
    "            self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        self.logger = logger\n",
    "        \n",
    "        # a list of pairs of the form (minibatch number, loss)\n",
    "        self.test_loss = [(0,-np.inf)]\n",
    "        \n",
    "        self.best_test_loss = (0,-np.inf)\n",
    "        \n",
    "        # the total number of minibatches run so far\n",
    "        self.minibatch_i = 0\n",
    "        \n",
    "        # list of indices giving minibatch counts at which training sessions\n",
    "        # have been run\n",
    "        self.training_minibatches = [0]\n",
    "        \n",
    "        # list of times for training sessions\n",
    "        self.training_times = [0.]\n",
    "        \n",
    "        self.best_V = self.model.V.get_value()\n",
    "        self.best_U = self.model.U.get_value()\n",
    "        self.best_W = self.model.W.get_value()\n",
    "        self.best_b = self.model.b.get_value()\n",
    "        \n",
    "        # mean gradient magnitudes - recorded every minibatch iteration\n",
    "        self.m_dV_mag = []\n",
    "        self.m_dU_mag = []\n",
    "        self.m_dW_mag = []\n",
    "        self.m_db_mag = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ```__init__``` does not build an RNN model, it expects one as input. RNN model building occurs in the following classmethod which implements initialization from a sentence npz file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_from_sentences_on_disk(cls,\n",
    "                                sentences_path,\n",
    "                                n_vocabulary,\n",
    "                                n_training_data,\n",
    "                                embedding_space_dim,\n",
    "                                minibatch_size,\n",
    "                                eval_test_loss_stride,\n",
    "                                n_test_loss_batches,\n",
    "                                nof1_dtype = np.int16,\n",
    "                                float_dtype = floatX,\n",
    "                                shuffle_seed = None,\n",
    "                                model_seed = None,\n",
    "                                embedder_seed = None,\n",
    "                                logger = logging\n",
    "                                ) :\n",
    "    \"\"\" word_embedder initialization from sentence data on disk\n",
    "\n",
    "        Inputs : \n",
    "\n",
    "        sentences_path        - file path to sentences .npz: sentences\n",
    "                                should start with '<s>' tag and end with \n",
    "                                one or more '<e>' tags\n",
    "        n_vocabulary          - vocabulary will be top n_vocabulary most\n",
    "                                common words in sentence data: will always\n",
    "                                include '<e>' and '<u>' tokens\n",
    "        n_training_data       - number of sentences in the training data\n",
    "        embedding_space_dim   - number of rows in input weights of RNN model \n",
    "        minibatch_size        - number of sentences in a minibatch\n",
    "        eval_test_loss_stride - number of minibatches between test loss\n",
    "                                evaluations\n",
    "        n_test_loss_batches   - number of batches over which to accumulate \n",
    "                                test loss \n",
    "        nof1_dtype            - integer type to use in storing one-of-n\n",
    "                                index arrays\n",
    "        float_dtype           - numeric type for use in rnn model weights\n",
    "        shuffle_seed          - seed for random shuffle of sentence data\n",
    "        model_seed            - seed for rnn model weight initialization\n",
    "        embedder_seed         - seed for random shuffle after each epoch\n",
    "        logger                - logging object\n",
    "\n",
    "        Output :\n",
    "\n",
    "        word_embedder instance\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # read the sentences from the disk\n",
    "    logger.info('reading sentences from %s'%sentences_path)\n",
    "    (sentences,\n",
    "     words,\n",
    "     counts,\n",
    "     vocabulary) = read_sentences(sentences_path,n_vocabulary)\n",
    "\n",
    "    # get the one of n indices\n",
    "    logger.info('computing one-of-n indices')\n",
    "    data = get_one_of_n_indices(vocabulary,sentences,dtype = nof1_dtype)\n",
    "\n",
    "    # shuffle the data\n",
    "\n",
    "    if shuffle_seed is not None : \n",
    "        np.random.seed(shuffle_seed)\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # partition the training and test data\n",
    "    training_data = data[:n_training_data]\n",
    "    test_data = data[n_training_data:]\n",
    "\n",
    "    # initialize the rnn model\n",
    "    logger.info('initializing embedding model')\n",
    "    model = embedding_model(n_vocabulary,\n",
    "                            embedding_space_dim,\n",
    "                            model_seed,\n",
    "                            int_dtype = nof1_dtype,\n",
    "                            float_dtype = float_dtype)\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    logger.info('init from sentences elapsed time = %fs'%elapsed)\n",
    "\n",
    "    return cls(vocabulary,\n",
    "               training_data,\n",
    "               test_data,\n",
    "               model,\n",
    "               minibatch_size,\n",
    "               eval_test_loss_stride,\n",
    "               n_test_loss_batches,\n",
    "               embedder_seed,\n",
    "               logger)\n",
    "\n",
    "# add the function to the class as a classmethod - this is\n",
    "# only needed because I am defining the class across\n",
    "# several Jupyter notebook cells\n",
    "word_embedder.init_from_sentences_on_disk = classmethod(init_from_sentences_on_disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the sentences are read, the one-of-n encoding and vocabulary are computed, the data is partitioned into training and test sets, and the RNN model is initialized. For fun I also included a random shuffle on the sentence data. \n",
    "\n",
    "I've mentioned several times that my first implementation of ```rnn``` was inefficient and led me to build in support for loss evaluation over batches. In light of the fact that accumulation turned out not to be necessary you may be wondering why I didn't get rid of it altogether, and avoid all this handwringing. My reasoning is that something like it may in fact be necessary when I start stacking RNN's up to build deep networks since inner layers will have to deal with dense vector inputs. \n",
    "\n",
    "Well without further ado, here is the loss accumulation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accumulate_loss(self) : \n",
    "    \"\"\" Accumulate the value of the loss function on the test data across\n",
    "        self.n_test_loss_batches batches\n",
    "\n",
    "        Output : \n",
    "\n",
    "        value of the loss function on the test data      \n",
    "    \"\"\"\n",
    "\n",
    "    self.logger.info(\"accumulating loss\")\n",
    "\n",
    "    self.model.loss_accum.set_value(0.)\n",
    "    self.model.loss_accum_i.set_value(0.)\n",
    "\n",
    "    for j in range(self.n_test_loss_batches) : \n",
    "\n",
    "        self.logger.info(\"batch %d of %d\"%(j+1,self.n_test_loss_batches))\n",
    "\n",
    "        j0 = j*self.test_loss_batch_size\n",
    "        j1 = (j+1)*self.test_loss_batch_size\n",
    "\n",
    "        batch = self.test_data[j0:j1]\n",
    "\n",
    "        self.model.loss(batch,batch)\n",
    "\n",
    "    self.logger.info(\"loss accumulation complete\")\n",
    "\n",
    "    return self.model.loss_accum.get_value()\n",
    "\n",
    "# bind the method to the class\n",
    "word_embedder.accumulate_loss = accumulate_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've made it the training method! \n",
    "\n",
    "SGD makes training easy: loop over minibatches; update the weights with a scalar multiple of the gradient; stop when you are satisfied that further improvements can no longer be made without overfitting. I have implemented three stopping criteria. First, if the magnitude of the difference between the latest loss evaluation and the previous loss evaluation is smaller that ```test_loss_epsilon``` then stop. This is meant to stop training if the test loss has stabilized. A more sophisticated approach would look at the variance of some number of the most recent test loss values. Second, if the mean magnitude of all the weight gradients is smaller than ```grad_mag_epsilon``` then stop. Certainly if all the graidents are small then we should be close to an extrema and further SGD updates will be ineffectual. Note that this doesn't really address the well-known vanishing/exploding gradients problem in RNNs. An approach for dealing with that is to track the singular values of the weight matrices. Third, if training exceeds ```max_seconds``` seconds in total runtime then stop. \n",
    "\n",
    "The following ```train``` method looks a bit more involved than it actually is. The core work of the method was described in the previous paragraph. It has been exploded a bit because I added status logging and am recording intermediate data (saving the mean gradient magnitudes and all the loss evaluations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(self,\n",
    "          eta,\n",
    "          n_minibatches,\n",
    "          test_loss_epsilon,\n",
    "          grad_mag_epsilon,\n",
    "          max_seconds\n",
    "          ) : \n",
    "    \"\"\" Perform a training session on the model\n",
    "\n",
    "        Input : \n",
    "\n",
    "        eta               - learning rate for SGD updates\n",
    "        n_minibatches     - number of minibatches to train over in this session\n",
    "        test_loss_epsilon - stop training if test loss falls below this\n",
    "        grad_mag_epsilon  - stop training if any of the mean gradient magnitudes \n",
    "                            falls below this\n",
    "        max_seconds       - stop training after this many seconds\n",
    "\n",
    "        Output : \n",
    "\n",
    "        stopping_code     - string indicating why training stopped\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    self.logger.info(\"training\")\n",
    "    self.logger.info(\"eta               = %17.17f\"%eta)\n",
    "    self.logger.info(\"test_loss_epsilon = %17.17f\"%test_loss_epsilon)\n",
    "    self.logger.info(\"grad_mag_epsilon  = %17.17f\"%grad_mag_epsilon)\n",
    "    self.logger.info(\"max_seconds       = %17.17f\"%max_seconds)\n",
    "\n",
    "    # the stopping code\n",
    "    stopping_code = 'Reached max number of minibatches (%d)'%n_minibatches\n",
    "\n",
    "    # compile the model if need be\n",
    "\n",
    "    self.model.compile_loss()\n",
    "    self.model.compile_sgd_update()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(self.minibatch_i,self.minibatch_i+n_minibatches) : \n",
    "\n",
    "        # get the minibatch\n",
    "\n",
    "        i0 = (i*self.minibatch_size)%self.training_data_size\n",
    "        i1 = ((i+1)*self.minibatch_size)%self.training_data_size\n",
    "\n",
    "        if i1 < i0 :\n",
    "            # we reached the end of the data\n",
    "            i1 = self.training_data_size\n",
    "\n",
    "        cnt += 1\n",
    "        self.logger.info('minibatch %d of %d, training indices [%d, %d]'%(cnt,n_minibatches,i0,i1))\n",
    "\n",
    "        minibatch = self.training_data[self.training_indices[i0:i1]]\n",
    "\n",
    "        # compute an update\n",
    "\n",
    "        sgd_i = float(self.minibatch_i)\n",
    "        self.minibatch_i += 1\n",
    "\n",
    "        self.logger.info('sgd update')\n",
    "\n",
    "        self.model.sgd_update(minibatch,minibatch,eta,sgd_i)\n",
    "\n",
    "        # save gradient magnitudes\n",
    "\n",
    "        self.m_dV_mag.append(self.model.m_dV_mag.get_value())\n",
    "        self.m_dU_mag.append(self.model.m_dU_mag.get_value())\n",
    "        self.m_dW_mag.append(self.model.m_dW_mag.get_value())\n",
    "        self.m_db_mag.append(self.model.m_db_mag.get_value())\n",
    "\n",
    "        self.logger.info('mean dV magnitude = %17.17f'%self.m_dV_mag[-1])\n",
    "        self.logger.info('mean dU magnitude = %17.17f'%self.m_dU_mag[-1])\n",
    "        self.logger.info('mean dW magnitude = %17.17f'%self.m_dW_mag[-1])\n",
    "        self.logger.info('mean db magnitude = %17.17f'%self.m_db_mag[-1])\n",
    "\n",
    "        # check convergence criteria\n",
    "\n",
    "        if i % self.eval_test_loss_stride == 0 : \n",
    "            # evaluate the test loss and check for 'convergence'\n",
    "\n",
    "            self.logger.info('evaluating test loss')\n",
    "\n",
    "            # accumulate the loss\n",
    "\n",
    "            prev_test_loss = self.test_loss[-1][1]\n",
    "\n",
    "            self.test_loss.append((self.minibatch_i,\n",
    "                                   self.accumulate_loss()))\n",
    "\n",
    "            self.logger.info('test loss = %17.17f'%self.test_loss[-1][1])\n",
    "\n",
    "            # save the weights and biases if the loss is the best yet\n",
    "            if self.test_loss[-1][1] >= self.best_test_loss[1] : \n",
    "\n",
    "                self.best_V = self.model.V.get_value()\n",
    "                self.best_U = self.model.U.get_value()\n",
    "                self.best_W = self.model.W.get_value()\n",
    "                self.best_b = self.model.b.get_value()\n",
    "\n",
    "                self.best_test_loss = (self.test_loss[-1][0],\n",
    "                                       self.test_loss[-1][1])\n",
    "\n",
    "            if np.fabs(prev_test_loss - self.test_loss[-1][1]) < test_loss_epsilon : \n",
    "\n",
    "                # record that we are stopping because of training loss\n",
    "                # stabilization\n",
    "                stopping_code = 'Training loss stabilized'\n",
    "\n",
    "                break\n",
    "\n",
    "        if (self.m_dV_mag[-1] < grad_mag_epsilon\n",
    "                and self.m_dU_mag[-1] < grad_mag_epsilon\n",
    "                and self.m_dW_mag[-1] < grad_mag_epsilon\n",
    "                and self.m_db_mag[-1] < grad_mag_epsilon) : \n",
    "            # the average gradient magnitudes are small so let's give up\n",
    "\n",
    "            self.logger.info('gradient magnitude convergence')\n",
    "\n",
    "            # accumulate the loss to see if we are better than the \n",
    "            # current best\n",
    "\n",
    "            if i % self.eval_test_loss_stride != 0 :\n",
    "\n",
    "                self.test_loss.append((self.minibatch_i,\n",
    "                                       self.accumulate_loss()))\n",
    "\n",
    "            # save the weights and biases if the loss is the best yet\n",
    "            if self.test_loss[-1][1] >= self.best_test_loss[1] : \n",
    "\n",
    "                self.best_V = self.model.V.get_value()\n",
    "                self.best_U = self.model.U.get_value()\n",
    "                self.best_W = self.model.W.get_value()\n",
    "                self.best_b = self.model.b.get_value()\n",
    "\n",
    "                self.best_test_loss = (self.test_loss[-1][0],\n",
    "                                       self.test_loss[-1][1])\n",
    "\n",
    "            # record that we stopped because of gradient magnitudes\n",
    "            stopping_code = 'Gradient magnitudes < %f'%grad_mag_epsilon\n",
    "\n",
    "            break\n",
    "\n",
    "        if i1 == self.training_data_size :\n",
    "            # shuffle the data for the next epoch\n",
    "            self.logger.info('epoch complete')\n",
    "            np.random.shuffle(self.training_indices)\n",
    "\n",
    "        # check if we have been running too long\n",
    "        elapsed = time.perf_counter() - start\n",
    "        self.logger.info('elpased time = %fs'%elapsed)\n",
    "        if elapsed > max_seconds : \n",
    "\n",
    "            stopping_code = \"Exceeded time limit of %fs. Elapsed = %fs\"%(max_seconds,elapsed)\n",
    "\n",
    "            break\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    self.training_minibatches.append(self.minibatch_i)\n",
    "    self.training_times.append(elapsed)\n",
    "\n",
    "    self.logger.info('stopping code = %s'%stopping_code)\n",
    "    self.logger.info('training run time %fs'%elapsed)\n",
    "    self.logger.info('total elapsed training run time %fs'%elapsed)\n",
    "    self.logger.info('ended at minibatch %d of %d'%(cnt,n_minibatches))\n",
    "    self.logger.info('total minibatches %d'%self.minibatch_i)\n",
    "    self.logger.info('test loss last evaluated at minibatch %d'%self.test_loss[-1][0])\n",
    "    self.logger.info('last test loss    = %17.17f'%self.test_loss[-1][1])\n",
    "    self.logger.info('mean dV magnitude = %17.17f'%self.m_dV_mag[-1])\n",
    "    self.logger.info('mean dU magnitude = %17.17f'%self.m_dU_mag[-1])\n",
    "    self.logger.info('mean dW magnitude = %17.17f'%self.m_dW_mag[-1])\n",
    "    self.logger.info('mean db magnitude = %17.17f'%self.m_db_mag[-1])\n",
    "\n",
    "    return stopping_code\n",
    "\n",
    "# bind the method to the class\n",
    "word_embedder.train = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is in place to run some training iterations on the very small example data set in sentences_sml.npz; it consists of 1000 sentences with 2305 unique words. Here is the embedder initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:reading sentences from ./sentences_sml.npz\n",
      "INFO:computing one-of-n indices\n",
      "INFO:initializing embedding model\n",
      "INFO:init from sentences elapsed time = 0.384053s\n"
     ]
    }
   ],
   "source": [
    "sentences_path = r'./sentences_sml.npz'\n",
    "n_vocabulary = 2300\n",
    "n_training_data = 800\n",
    "embedding_space_dim = 2\n",
    "minibatch_size = 400\n",
    "eval_test_loss_stride = 2\n",
    "n_test_loss_batches = 1\n",
    "nof1_dtype = 'int16'\n",
    "model_dtype = 'float32'\n",
    "shuffle_seed = 123\n",
    "model_seed = 456\n",
    "embedder_seed = 789\n",
    "\n",
    "embedder = word_embedder.init_from_sentences_on_disk(sentences_path,\n",
    "                                                     n_vocabulary,\n",
    "                                                     n_training_data,\n",
    "                                                     embedding_space_dim,\n",
    "                                                     minibatch_size,\n",
    "                                                     eval_test_loss_stride,\n",
    "                                                     n_test_loss_batches,\n",
    "                                                     nof1_dtype,\n",
    "                                                     model_dtype,\n",
    "                                                     shuffle_seed,\n",
    "                                                     model_seed,\n",
    "                                                     embedder_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've asked for the training data to have 800 sentences, leaving 200 sentences for the test set. I've set the number of embedding space dimensions to 2, which is very small but good enough as far as this notebook is concerned since I just want to show the code running a few training iterations. A minibatch will contain 400 sentences, so two minibatches is one epoch (a complete pass through the training data). By setting ```eval_test_loss_stride = 2```, I am requesting that the loss be computed every other training iteration (every 2 minibatchs). The variable ```n_test_loss_batches``` determines across how many batches the loss should be accumulated - as noted above this is a layover from the original  inefficient implementation of ```rnn```.\n",
    "\n",
    "Ok! We are ready to run some training iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training\n",
      "INFO:eta               = 0.10000000149011612\n",
      "INFO:test_loss_epsilon = 0.00010000000000000\n",
      "INFO:grad_mag_epsilon  = 0.00010000000000000\n",
      "INFO:max_seconds       = 36000.00000000000000000\n",
      "INFO:minibatch 1 of 4, training indices [0, 400]\n",
      "INFO:sgd update\n",
      "INFO:mean dV magnitude = 1.29920578002929688\n",
      "INFO:mean dU magnitude = 0.11843164265155792\n",
      "INFO:mean dW magnitude = 0.17303091287612915\n",
      "INFO:mean db magnitude = 0.16622026264667511\n",
      "INFO:evaluating test loss\n",
      "INFO:accumulating loss\n",
      "INFO:batch 1 of 1\n",
      "INFO:loss accumulation complete\n",
      "INFO:test loss = -8.29797744750976562\n",
      "INFO:elpased time = 2.708264s\n",
      "INFO:minibatch 2 of 4, training indices [400, 800]\n",
      "INFO:sgd update\n",
      "INFO:mean dV magnitude = 1.29996919631958008\n",
      "INFO:mean dU magnitude = 0.10756405442953110\n",
      "INFO:mean dW magnitude = 0.15645948052406311\n",
      "INFO:mean db magnitude = 0.15097820758819580\n",
      "INFO:epoch complete\n",
      "INFO:elpased time = 5.080980s\n",
      "INFO:minibatch 3 of 4, training indices [0, 400]\n",
      "INFO:sgd update\n",
      "INFO:mean dV magnitude = 1.29857373237609863\n",
      "INFO:mean dU magnitude = 0.09575548768043518\n",
      "INFO:mean dW magnitude = 0.13970348238945007\n",
      "INFO:mean db magnitude = 0.13542188704013824\n",
      "INFO:evaluating test loss\n",
      "INFO:accumulating loss\n",
      "INFO:batch 1 of 1\n",
      "INFO:loss accumulation complete\n",
      "INFO:test loss = -7.95047235488891602\n",
      "INFO:elpased time = 7.868911s\n",
      "INFO:minibatch 4 of 4, training indices [400, 800]\n",
      "INFO:sgd update\n",
      "INFO:mean dV magnitude = 1.29103422164916992\n",
      "INFO:mean dU magnitude = 0.08338762819766998\n",
      "INFO:mean dW magnitude = 0.12284819036722183\n",
      "INFO:mean db magnitude = 0.11954872310161591\n",
      "INFO:epoch complete\n",
      "INFO:elpased time = 10.231288s\n",
      "INFO:stopping code = Reached max number of minibatches (4)\n",
      "INFO:training run time 10.231784s\n",
      "INFO:total elapsed training run time 10.231784s\n",
      "INFO:ended at minibatch 4 of 4\n",
      "INFO:total minibatches 4\n",
      "INFO:test loss last evaluated at minibatch 3\n",
      "INFO:last test loss    = -7.95047235488891602\n",
      "INFO:mean dV magnitude = 1.29103422164916992\n",
      "INFO:mean dU magnitude = 0.08338762819766998\n",
      "INFO:mean dW magnitude = 0.12284819036722183\n",
      "INFO:mean db magnitude = 0.11954872310161591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Reached max number of minibatches (4)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = np.array(0.1,dtype=model_dtype)\n",
    "n_minibatches = 4\n",
    "test_loss_epsilon = 0.0001\n",
    "grad_mag_epsilon = 0.0001\n",
    "max_seconds = 60.\n",
    "\n",
    "embedder.train(eta,\n",
    "               n_minibatches,\n",
    "               test_loss_epsilon,\n",
    "               grad_mag_epsilon,\n",
    "               max_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And that's it! It's time to see what happens when I feed this thing 5 million sentences!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
